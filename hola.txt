2026-01-16 15:05:03.575 | WARNING: Using incubator modules: jdk.incubator.vector
2026-01-16 15:05:04.550 | Files local:///app/repo/k3s/spark/main.py from /app/.worktrees/7dac4753c1b175006ecd7c056aa0ecd2100224f5/k3s/spark/main.py to /opt/spark/work-dir/main.py
2026-01-16 15:05:04.609 | 26/01/16 21:05:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2026-01-16 15:05:05.261 | 2026-01-16 21:05:05,261 - SparkPreprocessing - DEBUG - Parameters retrieved from /app/repo/k3s/params.yaml
2026-01-16 15:05:05.776 | 2026-01-16 21:05:05,776 - SparkPreprocessing - INFO - Minio connection verified. Buckets: ['frontend-crm-bucket', 'k8s-mlops-platform-bucket']
2026-01-16 15:05:05.776 | 2026-01-16 21:05:05,776 - SparkPreprocessing - INFO - Creating SparkSession with S3A (MinIO) support
2026-01-16 15:05:06.004 | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
2026-01-16 15:05:06.006 | 26/01/16 21:05:06 INFO SparkContext: Running Spark version 4.0.1
2026-01-16 15:05:06.007 | 26/01/16 21:05:06 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64
2026-01-16 15:05:06.008 | 26/01/16 21:05:06 INFO SparkContext: Java version 17.0.17
2026-01-16 15:05:06.027 | 26/01/16 21:05:06 INFO ResourceUtils: ==============================================================
2026-01-16 15:05:06.027 | 26/01/16 21:05:06 INFO ResourceUtils: No custom resources configured for spark.driver.
2026-01-16 15:05:06.027 | 26/01/16 21:05:06 INFO ResourceUtils: ==============================================================
2026-01-16 15:05:06.029 | 26/01/16 21:05:06 INFO SparkContext: Submitted application: spark-preprocessing
2026-01-16 15:05:06.045 | 26/01/16 21:05:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2026-01-16 15:05:06.049 | 26/01/16 21:05:06 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
2026-01-16 15:05:06.050 | 26/01/16 21:05:06 INFO ResourceProfileManager: Added ResourceProfile id: 0
2026-01-16 15:05:06.085 | 26/01/16 21:05:06 INFO SecurityManager: Changing view acls to: spark
2026-01-16 15:05:06.085 | 26/01/16 21:05:06 INFO SecurityManager: Changing modify acls to: spark
2026-01-16 15:05:06.086 | 26/01/16 21:05:06 INFO SecurityManager: Changing view acls groups to: spark
2026-01-16 15:05:06.086 | 26/01/16 21:05:06 INFO SecurityManager: Changing modify acls groups to: spark
2026-01-16 15:05:06.088 | 26/01/16 21:05:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY; RPC SSL disabled
2026-01-16 15:05:06.320 | 26/01/16 21:05:06 INFO Utils: Successfully started service 'sparkDriver' on port 7078.
2026-01-16 15:05:06.344 | 26/01/16 21:05:06 INFO SparkEnv: Registering MapOutputTracker
2026-01-16 15:05:06.355 | 26/01/16 21:05:06 INFO SparkEnv: Registering BlockManagerMaster
2026-01-16 15:05:06.368 | 26/01/16 21:05:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2026-01-16 15:05:06.368 | 26/01/16 21:05:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2026-01-16 15:05:06.371 | 26/01/16 21:05:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2026-01-16 15:05:06.386 | 26/01/16 21:05:06 INFO DiskBlockManager: Created local directory at /var/data/spark-f7954b1e-75d6-432c-a4f4-b5f45dc400ed/blockmgr-3d9877cd-9325-45cf-a9eb-017cc4cf6089
2026-01-16 15:05:06.402 | 26/01/16 21:05:06 INFO SparkEnv: Registering OutputCommitCoordinator
2026-01-16 15:05:06.491 | 26/01/16 21:05:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
2026-01-16 15:05:06.592 | 26/01/16 21:05:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2026-01-16 15:05:06.645 | 26/01/16 21:05:06 WARN SparkContext: File with 'local' scheme local:/app/repo/k3s/spark/main.py is not supported to add to file server, since it is already available on every node.
2026-01-16 15:05:06.670 | 26/01/16 21:05:06 INFO SecurityManager: Changing view acls to: spark
2026-01-16 15:05:06.670 | 26/01/16 21:05:06 INFO SecurityManager: Changing modify acls to: spark
2026-01-16 15:05:06.671 | 26/01/16 21:05:06 INFO SecurityManager: Changing view acls groups to: spark
2026-01-16 15:05:06.671 | 26/01/16 21:05:06 INFO SecurityManager: Changing modify acls groups to: spark
2026-01-16 15:05:06.671 | 26/01/16 21:05:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY; RPC SSL disabled
2026-01-16 15:05:06.729 | 26/01/16 21:05:06 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
2026-01-16 15:05:08.112 | 26/01/16 21:05:08 INFO ExecutorPodsAllocator: Going to request 2 executors from Kubernetes for ResourceProfile Id: 0, target: 2, known: 0, sharedSlotFromPendingPods: 2147483647.
2026-01-16 15:05:08.172 | 26/01/16 21:05:08 INFO KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : spark.kubernetes.namespace
2026-01-16 15:05:08.372 | 26/01/16 21:05:08 INFO ExecutorPodsAllocator: Found 0 reusable PVCs from 0 PVCs
2026-01-16 15:05:08.414 | 26/01/16 21:05:08 INFO KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : spark.kubernetes.namespace
2026-01-16 15:05:08.415 | 26/01/16 21:05:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.
2026-01-16 15:05:08.419 | 26/01/16 21:05:08 INFO NettyBlockTransferService: Server created on spark-app-0-driver-svc.spark.svc 10.1.4.82:7079
2026-01-16 15:05:08.422 | 26/01/16 21:05:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2026-01-16 15:05:08.422 | 26/01/16 21:05:08 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script
2026-01-16 15:05:08.434 | 26/01/16 21:05:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-app-0-driver-svc.spark.svc, 7079, None)
2026-01-16 15:05:08.439 | 26/01/16 21:05:08 INFO BlockManagerMasterEndpoint: Registering block manager spark-app-0-driver-svc.spark.svc:7079 with 413.9 MiB RAM, BlockManagerId(driver, spark-app-0-driver-svc.spark.svc, 7079, None)
2026-01-16 15:05:08.441 | 26/01/16 21:05:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-app-0-driver-svc.spark.svc, 7079, None)
2026-01-16 15:05:08.442 | 26/01/16 21:05:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-app-0-driver-svc.spark.svc, 7079, None)
2026-01-16 15:05:08.531 | 26/01/16 21:05:08 INFO KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : spark.kubernetes.namespace
2026-01-16 15:05:08.532 | 26/01/16 21:05:08 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script
2026-01-16 15:05:13.171 | 26/01/16 21:05:13 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.1.4.84:40702) with ID 2, ResourceProfileId 0
2026-01-16 15:05:13.172 | 26/01/16 21:05:13 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.1.4.83:56232) with ID 1, ResourceProfileId 0
2026-01-16 15:05:13.250 | 26/01/16 21:05:13 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
2026-01-16 15:05:13.276 | 26/01/16 21:05:13 INFO BlockManagerMasterEndpoint: Registering block manager 10.1.4.83:44761 with 413.9 MiB RAM, BlockManagerId(1, 10.1.4.83, 44761, None)
2026-01-16 15:05:13.278 | 26/01/16 21:05:13 INFO BlockManagerMasterEndpoint: Registering block manager 10.1.4.84:41961 with 413.9 MiB RAM, BlockManagerId(2, 10.1.4.84, 41961, None)
2026-01-16 15:05:14.572 | 2026-01-16 21:05:14,572 - SparkPreprocessing - INFO - Loading feature pipeline: preprocessing.preprocessing_001
2026-01-16 15:05:14.717 | 2026-01-16 21:05:14,717 - SparkPreprocessing - INFO - Loading data from s3a://k8s-mlops-platform-bucket/v1/raw/train/
2026-01-16 15:05:14.766 | 26/01/16 21:05:14 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2026-01-16 15:05:14.771 | 26/01/16 21:05:14 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
2026-01-16 15:05:15.310 | 26/01/16 21:05:15 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
2026-01-16 15:05:15.317 | 26/01/16 21:05:15 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2026-01-16 15:05:15.317 | 26/01/16 21:05:15 INFO MetricsSystemImpl: s3a-file-system metrics system started
2026-01-16 15:05:17.042 | 26/01/16 21:05:17 INFO HadoopFSUtils: Listing s3a://k8s-mlops-platform-bucket/v1/raw/train with listFiles API
2026-01-16 15:05:17.218 | 26/01/16 21:05:17 INFO InMemoryFileIndex: It took 193 ms to list leaf files for 1 paths.
2026-01-16 15:05:17.456 | 26/01/16 21:05:17 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
2026-01-16 15:05:17.466 | 26/01/16 21:05:17 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
2026-01-16 15:05:17.468 | 26/01/16 21:05:17 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
2026-01-16 15:05:17.469 | 26/01/16 21:05:17 INFO DAGScheduler: Parents of final stage: List()
2026-01-16 15:05:17.473 | 26/01/16 21:05:17 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:05:17.477 | 26/01/16 21:05:17 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
2026-01-16 15:05:17.517 | 26/01/16 21:05:17 INFO MemoryStore: MemoryStore started with capacity 413.9 MiB
2026-01-16 15:05:17.531 | 26/01/16 21:05:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 119.3 KiB, free 413.8 MiB)
2026-01-16 15:05:17.560 | 26/01/16 21:05:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 43.4 KiB, free 413.8 MiB)
2026-01-16 15:05:17.566 | 26/01/16 21:05:17 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:05:17.584 | 26/01/16 21:05:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:05:17.594 | 26/01/16 21:05:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2026-01-16 15:05:17.624 | 26/01/16 21:05:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.1.4.84,executor 2, partition 0, PROCESS_LOCAL, 9840 bytes) 
2026-01-16 15:05:20.123 | 26/01/16 21:05:20 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (10.1.4.84 executor 2): org.apache.spark.sql.AnalysisException: [PARQUET_TYPE_ILLEGAL] Illegal Parquet type: INT64 (TIMESTAMP(NANOS,false)). SQLSTATE: 42846
2026-01-16 15:05:20.123 | 	at org.apache.spark.sql.errors.QueryCompilationErrors$.illegalParquetTypeError(QueryCompilationErrors.scala:2044)
2026-01-16 15:05:20.123 | 	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.illegalType$1(ParquetSchemaConverter.scala:232)
2026-01-16 15:05:20.123 | 	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convertPrimitiveField$2(ParquetSchemaConverter.scala:309)
2026-01-16 15:05:20.123 | 	at scala.Option.getOrElse(Option.scala:201)
2026-01-16 15:05:20.123 | 	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertPrimitiveField(ParquetSchemaConverter.scala:250)
2026-01-16 15:05:20.123 | 	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertField(ParquetSchemaConverter.scala:203)
2026-01-16 15:05:20.123 | 	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convertInternal$4(ParquetSchemaConverter.scala:163)
2026-01-16 15:05:20.123 | 	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convertInternal$4$adapted(ParquetSchemaConverter.scala:127)
2026-01-16 15:05:20.123 | 	at scala.collection.immutable.Range.map(Range.scala:60)
2026-01-16 15:05:20.123 | 	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertInternal(ParquetSchemaConverter.scala:127)
2026-01-16 15:05:20.123 | 	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convert(ParquetSchemaConverter.scala:85)
2026-01-16 15:05:20.123 | 	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readSchemaFromFooter$2(ParquetFileFormat.scala:515)
2026-01-16 15:05:20.123 | 	at scala.Option.getOrElse(Option.scala:201)
2026-01-16 15:05:20.123 | 	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readSchemaFromFooter(ParquetFileFormat.scala:515)
2026-01-16 15:05:20.123 | 	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$2(ParquetFileFormat.scala:495)
2026-01-16 15:05:20.123 | 	at scala.collection.immutable.List.map(List.scala:247)
2026-01-16 15:05:20.123 | 	at scala.collection.immutable.List.map(List.scala:79)
2026-01-16 15:05:20.123 | 	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:495)
2026-01-16 15:05:20.123 | 	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:486)
2026-01-16 15:05:20.123 | 	at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
2026-01-16 15:05:20.123 | 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)
2026-01-16 15:05:20.123 | 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)
2026-01-16 15:05:20.123 | 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
2026-01-16 15:05:20.123 | 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
2026-01-16 15:05:20.123 | 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
2026-01-16 15:05:20.123 | 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2026-01-16 15:05:20.123 | 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
2026-01-16 15:05:20.123 | 	at org.apache.spark.scheduler.Task.run(Task.scala:147)
2026-01-16 15:05:20.123 | 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
2026-01-16 15:05:20.123 | 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
2026-01-16 15:05:20.123 | 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
2026-01-16 15:05:20.123 | 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
2026-01-16 15:05:20.123 | 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
2026-01-16 15:05:20.123 | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
2026-01-16 15:05:20.123 | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
2026-01-16 15:05:20.123 | 	at java.base/java.lang.Thread.run(Thread.java:840)
2026-01-16 15:05:20.123 | 
2026-01-16 15:05:20.125 | 26/01/16 21:05:20 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 1) (10.1.4.83,executor 1, partition 0, PROCESS_LOCAL, 9840 bytes) 