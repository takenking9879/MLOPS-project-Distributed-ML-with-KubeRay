apiVersion: ray.io/v1
kind: RayJob
metadata:
  name: kuberay-job
  namespace: ray
spec:
  entrypoint: python3 app/repo/k3s/kuberay/main.py

  # shutdownAfterJobFinishes specifies whether the RayCluster should be deleted after the RayJob finishes. Default is false.
  shutdownAfterJobFinishes: true

  # ttlSecondsAfterFinished specifies the number of seconds after which the RayCluster will be deleted after the RayJob finishes.
  ttlSecondsAfterFinished: 20

  runtimeEnvYAML: |
    pip:
      - xgboost==3.1.3
      - pyarrow==22.0.0
      - python-dotenv==1.2.1
      - numpy==2.3.3
      - pandas==2.3.3
      - scikit-learn==1.8.0
      - torch==2.9.1
      - mlflow==3.6.0
      - PyYAML==6.0.3
      - boto3==1.42.28
    env_vars:
      # Workers de Ray Train por *trial*.
      # Usado tanto para entrenamiento (modules/) como para tuning (tuning/).
      NUM_WORKERS: "2"

      # Núcleos de CPU por cada worker de Ray Train.
      # REGLA IMPORTANTE (Tune + PlacementGroupFactory):
      # Si tu código de tuning envuelve un Trainer con:
      #   PlacementGroupFactory([{"CPU": 1}, {"CPU": CPUS_PER_WORKER} * NUM_WORKERS])
      # entonces CPUS_PER_WORKER aquí DEBE coincidir con el CPU del bundle del worker.
      # Si no coinciden, los trials pueden quedarse en PENDING (deadlock) porque
      # Ray Train no puede programar sus workers en el placement group.
      CPUS_PER_WORKER: "2"

      # Modelo mental de concurrencia:
      # - Cada trial de Tune solicita un placement group (PG). Los trials solo arrancan cuando el PG completo cabe.
      # - Si hay menos CPUs en el clúster de las necesarias para 2 trials, Tune ejecuta los trials secuencialmente.
      # - ¿Puedes tener menos workers que experimentos?: SÍ (los trials se encolan).
      # - Si tienes más workers que experimentos: los CPUs extra quedan inactivos a menos que se aumenten los recursos por trial.
  # evito usar porque me gusta más gitsync xd que working_dir: "https://github.com/ray-project/kuberay/archive/master.zip"

  # rayClusterSpec specifies the RayCluster instance to be created by the RayJob controller.
  rayClusterSpec:
    rayVersion: '2.52.0'
    enableInTreeAutoscaling: true
    autoscalerOptions:
      version: v2
      upscalingMode: Default
      idleTimeoutSeconds: 60
    headGroupSpec:
      rayStartParams:
        num-cpus: "0"
      # Pod template
      template:
        spec:
          containers:
          - name: ray-head
            image: rayproject/ray:2.52.0
            ports:
            - containerPort: 6379
              name: gcs-server
            - containerPort: 8265 # Ray dashboard
              name: dashboard
            - containerPort: 10001
              name: client
            resources:
              limits:
                cpu: "1"
                memory: "5Gi"
              requests:
                cpu: "1"
                memory: "4Gi"
            # NOTE:
            # Tune/Train trial drivers typically run on the head too.
            # If head has only 1 CPU, and your PlacementGroupFactory reserves 1 CPU for the head bundle,
            # the cluster can still work, but it leaves little room for extra head-side work.
            volumeMounts:
              - name: job-files
                mountPath: /app
            envFrom:
              - secretRef:
                  name: env-secret
          initContainers:
          - name: git-sync
            image: registry.k8s.io/git-sync/git-sync:v4.1.0
            resources:
              requests:
                cpu: 100m
                memory: 128Mi
              limits:
                cpu: 200m
                memory: 256Mi
            env:
              - name: GITSYNC_REPO
                value: "https://github.com/takenking9879/Kubernetes-MLOPS-platform"
              - name: GIT_SYNC_BRANCH
                value: "main"
              - name: GIT_SYNC_ROOT
                value: "/git"
              - name: GIT_SYNC_DEST
                value: "repo"
              - name: GIT_SYNC_WAIT
                value: "30"
              - name: GITSYNC_ONE_TIME
                value: "true"
            volumeMounts:
              - name: job-files
                mountPath: /git
        volumes:
          - name: job-files
            emptyDir: {}

    workerGroupSpecs:
    - replicas: 2
      minReplicas: 2
      maxReplicas: 3
      groupName: small-group
      rayStartParams: {}
      # Pod template
      template:
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  podAffinityTerm:
                    labelSelector:
                      matchExpressions:
                        - key: ray.io/group
                          operator: In
                          values:
                            - small-group
                    topologyKey: "kubernetes.io/hostname"
          containers:
          - name: ray-worker
            image: rayproject/ray:2.52.0
            resources:
              limits:
                cpu: "4" # Siempre dejarlo un cpu más alto que CPUS_PER_WORKER y requests
                memory: "4Gi"
              requests:
                cpu: "3" # Siempre dejarlo un cpu más alto que CPUS_PER_WORKER
                memory: "4Gi"
            # PlacementGroupFactory RULE:
            # - Each bundle must fit on ONE node.
            # - If CPUS_PER_WORKER > this worker CPU, the bundle is infeasible and trials will never start.
            # - If CPUS_PER_WORKER is much smaller than this worker CPU, you can run more concurrent trials.
            volumeMounts:
              - name: job-files
                mountPath: /app
            envFrom:
              - secretRef:
                  name: env-secret

          initContainers:
          - name: git-sync
            image: registry.k8s.io/git-sync/git-sync:v4.1.0
            resources:
              requests:
                cpu: 100m
                memory: 128Mi
              limits:
                cpu: 200m
                memory: 256Mi
            env:
              - name: GITSYNC_REPO
                value: "https://github.com/takenking9879/Kubernetes-MLOPS-platform"
              - name: GIT_SYNC_BRANCH
                value: "main"
              - name: GIT_SYNC_ROOT
                value: "/git"
              - name: GIT_SYNC_DEST
                value: "repo"
              - name: GIT_SYNC_WAIT
                value: "30"
              - name: GITSYNC_ONE_TIME
                value: "true"
            volumeMounts:
              - name: job-files
                mountPath: /git
        volumes:
          - name: job-files
            emptyDir: {}
