operatorRbac:
  serviceAccount:
    create: true
    name: "spark-operator"
  clusterRole:
    create: true
    name: "spark-operator-clusterrole"
  clusterRoleBinding:
    create: true
    name: "spark-operator-clusterrolebinding"
  role:
    # If enabled, a Role would be created inside each workload namespace, as configured
    # in {workloadResources.namespaces.data} for operator. Please enable *at least one* of
    # {operatorRbac.clusterRole.create} or {operatorRbac.role.create} for operator
    # unless permission is provided separately from the chart
    create: true
    name: "spark-operator-role"
  roleBinding:
    # If enabled, a RoleBinding would be created inside each workload namespace, as configured
    # in {workloadResources.namespaces.data} for operator. Please enable *at least one* of
    # {operatorRbac.clusterRoleBinding.create} or {operatorRbac.roleBinding.create} for
    # operator unless permission is provided separately from the chart
    create: true
    name: "spark-operator-rolebinding"
    # It's possible to make the rolebinding refers to role / clusterrole as applicable
    roleRef:
      kind: Role
      name: "spark-operator-role"
  configManagement:
    # Role to enable operator loading hot properties from config map
    create: true
    roleName: "spark-operator-config-monitor"
    roleBindingName: "spark-operator-config-monitor-role-binding"
  labels:
    "app.kubernetes.io/component": "operator-rbac"

workloadResources:
  # Create namespace(s), service account(s), clusterrole, role(s) and rolebinding(s) for Spark
  # workloads, including SparkApps and SparkClusters
  namespaces:
    create: true
    # When enabled, value for conf property `spark.operator.watched.namespaces` would be set to
    # the values provided in {workloadResources.namespaces.data} field as well
    overrideWatchedNamespaces: true
    data:
    # - "spark-1"
    # - "spark-2"
  serviceAccount:
    create: true
    name: "spark"
    namespace: "spark" #Para que el serviceAccount sea en el namespace spark
  role:
    # When enabled, a role would be created in each workload namespace, as configured in
    # {workloadResources.namespaces.data} for Spark apps. Please enable *at least one* of
    # {workloadResources.roles.create} or {workloadResources.clusterRole.create} for
    # Spark workload unless permission is provided separately from the chart
    create: true
    name: "spark-workload-role"
  clusterRole:
    # When enabled, a clusterrole would be created for Spark workloads. Please enable *at least one* of
    # {workloadResources.roles.create} or {workloadResources.clusterRole.create} for Spark workload unless
    # permission is provided separately from the chart
    create: true
    name: "spark-workload-clusterrole"
  roleBinding:
    create: true
    name: "spark-workload-rolebinding"
  sparkApplicationSentinel:
    create: false
    name: "spark-app-sentinel"
    sentinelNamespaces:
      data:
      #  - "spark-1"
      #  - "spark-2"
      # When enabled, a sentinel resource of type SparkApp will be deployed to namespace(s) provided
      # Note that sentinelNamespaces list should be a subset of {workloadResources.namespaces.data}
      # - "default"
  sparkClusterSentinel:
    create: false
    name: "spark-cluster-sentinel"
    sentinelNamespaces:
      data:
      #  - "spark-1"
      #  - "spark-2"
      # When enabled, a sentinel resource of type SparkCluster will be deployed to namespace(s) provided
      # Note that sentinelNamespaces list should be a subset of {workloadResources.namespaces.data}
      # - "default"
  # workload resources are by default annotated to avoid workload abort due to operator upgrade
  annotations:
    "helm.sh/resource-policy": keep
  # labels to be added on workload resources
  labels:
    "app.kubernetes.io/component": "spark-workload"

operatorConfiguration:
  # If set to true, below properties would be appended to default conf files under conf/
  # Otherwise, below would override default conf files
  append: true
  # Labels and annotations for the operatorConfiguration config map
  configMap:
    annotations:
    labels:
  log4j2.properties: |+
    # Logging Overrides
    rootLogger.level=INFO
    rootLogger.appenderRef.stdout.ref = console
    appender.console.type = Console
    appender.console.name = console
    appender.console.target = SYSTEM_ERR
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} %p %X{resource.name} %X{resource.namespace} %C{1.} %m%n%ex
  spark-operator.properties: |+
    # Property Overrides. e.g.
    # spark.kubernetes.operator.reconciler.intervalSeconds=60
  metrics.properties: |+
    # Metrics Properties Overrides
  dynamicConfig:
    # Enable this for hot properties loading.
    enable: false
    # Enable this to create a config map for hot property loading
    create: false
    annotations:
      "helm.sh/resource-policy": keep
    data:
      # Spark Operator Config Runtime Properties Overrides. e.g.
      spark.kubernetes.operator.reconciler.intervalSeconds: "60"
